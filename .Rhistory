## Optionally, rename to more convenient column names
sampleInfo <- dplyr::rename(sampleInfo, "group" = "histology:ch1", "patient"="characteristics_ch1.4")
library(pheatmap)
corMatrix <- cor(exprs(gse),use="c")
pheatmap(corMatrix)
rownames(sampleInfo) <- colnames(corMatrix)
pheatmap(corMatrix,
annotation_col=sampleInfo)
plotMDS(gse, labels=sampleInfo[,"group"],
gene.selection="common")
#Plot principal components labeled by group
pca <- prcomp(t(edata))
## Join the PCs to the sample information
cbind(sampleInfo, pca$x) %>%
ggplot(aes(x = PC1, y=PC2, col=group,label=paste("Patient", patient))) + geom_point() + geom_text_repel()
design <- model.matrix(~0+sampleInfo$group)
colnames(design) <- c("adenocarcinoma","non_malignant") #rename columns
kable(head(design), booktabs=T)%>%
kable_styling(latex_options = "hold_position")
contrasts <- makeContrasts(adenocarcinoma - non_malignant, levels=design)
contrasts
fit <- lmFit(exprs(gse), design)
contrasts <- makeContrasts(adenocarcinoma - non_malignant, levels=design)
fit2 <- contrasts.fit(fit,contrasts)
efit <- eBayes(fit2) #apply empirical Bayes to get the differential expression statistics and p-values
kable(topTable(efit), booktabs=T)%>%
kable_styling(latex_options = "hold_position")
results <- (decideTests(efit))
kable(summary(results), booktabs=T)
anno <- fData(gse)
anno <- select(anno,Symbol, Entrez_Gene_ID,Chromosome,Cytoband)
efit$genes <- anno
kable(topTable(efit), booktabs=T)%>%
kable_styling(latex_options= c("scale_down", "hold_position"))
full_results <- topTable(efit, number=Inf)
full_results <- tibble::rownames_to_column(full_results, "ID")
ggplot(full_results, aes(x=logFC, y=B)) +
geom_point()
p_cutoff <- 0.05
fc_cutoff <- 1
topN <- 20
full_results %>%
mutate(Significant = adj.P.Val < p_cutoff, abs(logFC) > fc_cutoff) %>%
mutate(Rank= 1:n(), Label= ifelse(Rank < topN, Symbol, "")) %>%
ggplot(aes(x = logFC, y=B, col=Significant, label= Label)) +
geom_text_repel(col="black")
#visualise
plotSA(efit, main="Final model: Mean-variance trend")
kable(topTable(efit), booktabs=T)%>%
kable_styling(latex_options=c("scale_down", "hold_position"))
tfit <- treat(efit, lfc=1)
dt <- decideTests(tfit)
kable(summary(dt), booktabs=T)%>%
kable_styling(latex_options= "hold_position")
adeno_vs_non <- topTreat(tfit, coef=1, n=Inf)
kable(head(adeno_vs_non), booktabs=T) %>%
kable_styling(latex_options= c("scale_down", "hold_position"))
plotMD(tfit, column=1, status=dt[,1], main=colnames(tfit)[1],
xlim=c(2,16))
kable(filter(full_results, Symbol == "SLC22A1"), booktabs=T)%>%
kable_styling(latex_options=c("scale_down", "hold_position"))
kable(filter(full_results, Symbol == "SLC22A4"), booktabs=T)%>%
kable_styling(latex_options=c("scale_down", "hold_position"))
kable(filter(full_results, Symbol == "SLC22A5"), booktabs=T)%>%
kable_styling(latex_options=c("scale_down", "hold_position"))
kable(filter(full_results, grepl("SLC22", Symbol)), booktabs=T)%>%
kable_styling(latex_options=c("scale_down", "hold_position"))
group=as.factor(sampleInfo$group)
##### GLIMMA ########3
#BiocManager::install("Glimma")
library(Glimma)
library(edgeR)
cpm <- cpm(edata)
lcpm <- cpm(edata, log=TRUE)
d <-glMDPlot(tfit, coef=1, status=dt, main=colnames(tfit)[1],
side.main="Entrez_Gene_ID", counts=edata, groups=group, path = "..", launch=TRUE)
d
library(readr)
filter(full_results, adj.P.Val < 0.05, abs(logFC) >1) %>%
write.csv(file ="./results/filtered_GSE75037.csv")
sessionInfo()
library(htmlwidgets)
htmlwidgets::saveWidget(d, "GSE75037-plot.html", selfcontained = T)
library(htmlwidgets)
htmlwidgets::saveWidget(d, "gse75-plot.html", selfcontained = T)
saveWidget(d, "gse75-plot.html", selfcontained = T)
d <-glMDPlot(tfit, coef=1, status=dt, main=colnames(tfit)[1],
side.main="Entrez_Gene_ID", counts=edata, groups=group, path = ".",folder = "glimma-plots", html = "GSE75037-Plot", launch=TRUE)
setwd("~/Documents/Github/Quantium_Data Analytics_Virtual_Experience_Program")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(linewidth=80)
library(data.table)
library(ggplot2)
library(tidyr)
```{r 1. Read in data from previous module}
filePath <- ""
data <- fread(paste0(filePath,"QVI_data.csv"))
data <- fread(paste0(filePath,"QVI_data_task2.csv"))
#### Set themes for plots
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = 0.5))
data[, YEARMONTH := year(DATE)*100 + month(DATE)]
View(data)
# For each store and month calculate total sales, number of customers, transactions per customer, chips per customer and the average price per unit.
## Hint: you can use uniqueN() to count distinct values in a column
measureOverTime <- data[, .(totSales = sum(TOT_SALES),
nCustomers = unique(LYLTY_CARD_NBR),
nTxnPerCust = unique(TXN_ID)/ unique(LYLTY_CARD_NBR),
nChipsPerTxn = sum(PROD_QTY) / unique(TXN_ID) ,
avgPricePerUnit = sum(TOT_SALES)/sum(PROD_QTY))
, by = c("STORE_NBR", "YEARMONTH")][order(STORE_NBR, YEARMONTH) ]
# For each store and month calculate total sales, number of customers, transactions per customer, chips per customer and the average price per unit.
## Hint: you can use uniqueN() to count distinct values in a column
measureOverTime <- data[, .(totSales = sum(TOT_SALES),
nCustomers = unique(LYLTY_CARD_NBR),
nTxnPerCust = unique(TXN_ID)/ unique(LYLTY_CARD_NBR),
nChipsPerTxn = sum(PROD_QTY) / unique(TXN_ID) ,
avgPricePerUnit = sum(TOT_SALES)/sum(PROD_QTY) ), by = c("STORE_NBR", "YEARMONTH")][order(STORE_NBR, YEARMONTH) ]
# For each store and month calculate total sales, number of customers, transactions per customer, chips per customer and the average price per unit.
## Hint: you can use uniqueN() to count distinct values in a column
measureOverTime <- data[, .(totSales = sum(TOT_SALES),
nCustomers = unique(LYLTY_CARD_NBR),
nTxnPerCust = unique(TXN_ID)/ unique(LYLTY_CARD_NBR),
nChipsPerTxn = sum(PROD_QTY) / unique(TXN_ID) ,
avgPricePerUnit = sum(TOT_SALES)/sum(PROD_QTY) ),
by = c("STORE_NBR", "YEARMONTH")][order(STORE_NBR, YEARMONTH) ]
# For each store and month calculate total sales, number of customers, transactions per customer, chips per customer and the average price per unit.
## Hint: you can use uniqueN() to count distinct values in a column
measureOverTime <- data[, .(totSales = sum(TOT_SALES),
nCustomers = uniqueN(LYLTY_CARD_NBR) ,
nTxnPerCust = uniqueN(TXN_ID)/uniqueN(LYLTY_CARD_NBR),
nChipsPerTxn = sum(PROD_QTY)/uniqueN(TXN_ID),
avgPricePerUnit = sum(TOT_SALES)/sum(PROD_QTY))
, by = c("STORE_NBR", "YEARMONTH")][order(STORE_NBR, YEARMONTH) ]
#### Filter to the pre-trial period and stores with full observation periods
storesWithFullObs <- unique(measureOverTime[, .N, STORE_NBR][N == 12, STORE_NBR])
preTrialMeasures <- measureOverTime[YEARMONTH < 201902 & STORE_NBR %in%
storesWithFullObs, ]
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <- function(inputTable, metricCol, storeComparison) {
calcCorrTable = data.table(Store1 = numeric(), Store2 = numeric(), corr_measure =
numeric())
storeNumbers <-
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = ,
"Store2" = ,
"corr_measure" =
)
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
}
return(calcCorrTable)
}
#### Create a function to calculate a standardised magnitude distance for a measure,
#### looping through each control store
calculateMagnitudeDistance <- function(inputTable, metricCol, storeComparison) {
calcDistTable = data.table(Store1 = numeric(), Store2 = numeric(), YEARMONTH =
numeric(), measure = numeric())
storeNumbers <- unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison
, "Store2" = i
, "YEARMONTH" = inputTable[STORE_NBR ==
storeComparison, YEARMONTH]
, "measure" = abs(inputTable[STORE_NBR ==
storeComparison, eval(metricCol)]
- inputTable[STORE_NBR == i,
eval(metricCol)])
)
calcDistTable <- rbind(calcDistTable, calculatedMeasure)
}
#### Standardise the magnitude distance so that the measure ranges from 0 to 1
minMaxDist <- calcDistTable[, .(minDist = min(measure), maxDist = max(measure)),
by = c("Store1", "YEARMONTH")]
distTable <- merge(calcDistTable, minMaxDist, by = c("Store1", "YEARMONTH"))
distTable[, magnitudeMeasure := 1 - (measure - minDist)/(maxDist - minDist)]
finalDistTable <- distTable[, .(mag_measure = mean(magnitudeMeasure)), by =
.(Store1, Store2)]
return(finalDistTable)
}
#### Over to you! Use the function you created to calculate correlations against store 77 using total sales and number of customers.
#### Hint: Refer back to the input names of the functions we created.
trial_store <- 77
corr_nSales <- calculateCorrelation(preTrialMeasures, quote(totSales), trial_store)
corr_nSales <- calculateCorrelation(, quote(), )
corr_nSales <- calculateCorrelation(data, quote(), trial_store)
corr_nSales <- calculateCorrelation(preTrialMeasures, quote(totSales), trial_store)
#### Over to you! Use the function you created to calculate correlations against store 77 using total sales and number of customers.
#### Hint: Refer back to the input names of the functions we created.
trial_store <- 77
corr_nSales <- calculateCorrelation(preTrialMeasures, quote(totSales), trial_store)
corr_nSales <‐ calculateCorrelation(preTrialMeasures, quote(totSales), trial_store)
corr_nSales <- calculateCorrelation(preTrialMeasures, quote(totSales), trial_store)
# For each store and month calculate total sales, number of customers, transactions per customer, chips per customer and the average price per unit.
## Hint: you can use uniqueN() to count distinct values in a column
measureOverTime <- data[, .(totSales = sum(TOT_SALES),
nCustomers = uniqueN(LYLTY_CARD_NBR) ,
nTxnPerCust = uniqueN(TXN_ID)/uniqueN(LYLTY_CARD_NBR),
nChipsPerTxn = sum(PROD_QTY)/uniqueN(TXN_ID),
avgPricePerUnit = sum(TOT_SALES)/sum(PROD_QTY))
, by = c("STORE_NBR", "YEARMONTH")][order(STORE_NBR, YEARMONTH) ]
# For each store and month calculate total sales, number of customers, transactions per customer, chips per customer and the average price per unit.
## Hint: you can use uniqueN() to count distinct values in a column
measureOverTime <- data[, .(totSales = sum(TOT_SALES),
nCustomers = uniqueN(LYLTY_CARD_NBR) ,
nTxnPerCust = uniqueN(TXN_ID)/uniqueN(LYLTY_CARD_NBR),
nChipsPerTxn = sum(PROD_QTY)/uniqueN(TXN_ID),
avgPricePerUnit = sum(TOT_SALES)/sum(PROD_QTY))
, by = c("STORE_NBR", "YEARMONTH")][order(STORE_NBR, YEARMONTH) ]
#### Filter to the pre-trial period and stores with full observation periods
storesWithFullObs <- unique(measureOverTime[, .N, STORE_NBR][N == 12, STORE_NBR])
#### Filter to the pre-trial period and stores with full observation periods
storesWithFullObs <- unique(measureOverTime[, .N, STORE_NBR][N == 12, STORE_NBR])
preTrialMeasures <- measureOverTime[YEARMONTH < 201902 & STORE_NBR %in% storesWithFullObs, ]
Now we need to work out a way of ranking how similar each potential control store
is to the trial store. We can calculate how correlated the performance of each
store is to the trial store.
Let's write a function for this so that we don't have to calculate this for each
trial store and control store pair.
```{r Create function to calculate correlation}
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <- function(inputTable, metricCol, storeComparison) {
calcCorrTable = data.table(Store1 = numeric(), Store2 = numeric(), corr_measure =
numeric())
storeNumbers <-
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = ,
"Store2" = ,
"corr_measure" =
)
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
}
return(calcCorrTable)
}
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
Apart from correlation, we can also calculate a standardised metric based on the
absolute difference between the trial store's performance and each control store's
performance.
Let's write a function for this.
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <- function(inputTable, metricCol, storeComparison) {
calcCorrTable = data.table(Store1 = numeric(), Store2 = numeric(), corr_measure = numeric())
storeNumbers <-unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison, "Store2" = i,
"corr_measure" = cor(inputTable[STORE_NBR == storeComparison, eval(metricCol)], inputTable[STORE_NBR == i, eval(metricCol)]))
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
}
return(calcCorrTable)
}
calculatedMeasure = data.table("Store1" = storeComparison,
"Store2" = i,
"YEARMONTH" = inputTable[STORE_NBR == storeComparison,
YEARMONTH],
"measure" = abs(inputTable[STORE_NBR == storeComparison, eval(metricCol)] - inputTable[STORE_NBR == i,
eval(metricCol)])
)
#### Create a function to calculate a standardised magnitude distance for a measure,
#### looping through each control store
calculateMagnitudeDistance <- function(inputTable, metricCol, storeComparison) {
calcDistTable = data.table(Store1 = numeric(), Store2 = numeric(), YEARMONTH =
numeric(), measure = numeric())
storeNumbers <- unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison,
"Store2" = i,
"YEARMONTH" = inputTable[STORE_NBR == storeComparison,
YEARMONTH],
"measure" = abs(inputTable[STORE_NBR == storeComparison, eval(metricCol)] - inputTable[STORE_NBR == i,
eval(metricCol)])
)
calcDistTable <- rbind(calcDistTable, calculatedMeasure)
}
#### Standardise the magnitude distance so that the measure ranges from 0 to 1
minMaxDist <- calcDistTable[, .(minDist = min(measure), maxDist = max(measure)),
by = c("Store1", "YEARMONTH")]
distTable <- merge(calcDistTable, minMaxDist, by = c("Store1", "YEARMONTH"))
distTable[, magnitudeMeasure := 1 - (measure - minDist)/(maxDist - minDist)]
finalDistTable <- distTable[, .(mag_measure = mean(magnitudeMeasure)), by =
.(Store1, Store2)]
return(finalDistTable)
}
#### Over to you! Use the function you created to calculate correlations against store 77 using total sales and number of customers.
#### Hint: Refer back to the input names of the functions we created.
trial_store <- 77
corr_nSales <- calculateCorrelation(preTrialMeasures, quote(totSales), trial_store)
#### Filter to the pre-trial period and stores with full observation periods
storesWithFullObs <- unique(measureOverTime[, .N, STORE_NBR][N == 12, STORE_NBR])
preTrialMeasures <- measureOverTime[YEARMONTH < 201902 & STORE_NBR %in% storesWithFullObs, ]
Now we need to work out a way of ranking how similar each potential control store
is to the trial store. We can calculate how correlated the performance of each
store is to the trial store.
Let's write a function for this so that we don't have to calculate this for each
trial store and control store pair.
```{r Create function to calculate correlation}
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <- function(inputTable, metricCol, storeComparison) {
calcCorrTable = data.table(Store1 = numeric(), Store2 = numeric(), corr_measure = numeric())
storeNumbers <-unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison, "Store2" = i,
"corr_measure" = cor(inputTable[STORE_NBR == storeComparison, eval(metricCol)], inputTable[STORE_NBR == i, eval(metricCol)]))
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
}
return(calcCorrTable)
}
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
calcCorrTable <‐ rbind(calcCorrTable, calculatedMeasure)
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <- function(inputTable, metricCol, storeComparison) {
calcCorrTable = data.table(Store1 = numeric(), Store2 = numeric(), corr_measure = numeric())
storeNumbers <-unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison, "Store2" = i,
"corr_measure" = cor(inputTable[STORE_NBR == storeComparison, eval(metricCol)], inputTable[STORE_NBR == i, eval(metricCol)]))
calcCorrTable <‐ rbind(calcCorrTable, calculatedMeasure)
return(calcCorrTable)
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <- function(inputTable, metricCol, storeComparison) {
calcCorrTable = data.table(Store1 = numeric(), Store2 = numeric(), corr_measure = numeric())
storeNumbers <-unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison, "Store2" = i,
"corr_measure" = cor(inputTable[STORE_NBR == storeComparison, eval(metricCol)], inputTable[STORE_NBR == i, eval(metricCol)]))
calcCorrTable <‐ rbind(calcCorrTable, calculatedMeasure)
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <‐ function(inputTable, metricCol, storeComparison) {
#### Create a function to calculate correlation for a measure, looping through each control store.
#### Let's define inputTable as a metric table with potential comparison stores, metricCol as the store metric used to calculate correlation on, and storeComparison as the store number of the trial store.
calculateCorrelation <- function(inputTable, metricCol, storeComparison) {
calcCorrTable = data.table(Store1 = numeric(), Store2 = numeric(), corr_measure  = numeric())
storeNumbers <- unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison, "Store2" = i,
"corr_measure" = cor(inputTable[STORE_NBR == storeComparison,
eval(metricCol)], inputTable[STORE_NBR == i, eval(metricCol)]))
calcCorrTable <- rbind(calcCorrTable, calculatedMeasure)
}
return(calcCorrTable)
}
#### Create a function to calculate a standardised magnitude distance for a measure,
#### looping through each control store
calculateMagnitudeDistance <- function(inputTable, metricCol, storeComparison) {
calcDistTable = data.table(Store1 = numeric(), Store2 = numeric(), YEARMONTH =
numeric(), measure = numeric())
storeNumbers <- unique(inputTable[, STORE_NBR])
for (i in storeNumbers) {
calculatedMeasure = data.table("Store1" = storeComparison,
"Store2" = i,
"YEARMONTH" = inputTable[STORE_NBR == storeComparison,
YEARMONTH],
"measure" = abs(inputTable[STORE_NBR == storeComparison, eval(metricCol)] - inputTable[STORE_NBR == i,
eval(metricCol)])
)
calcDistTable <- rbind(calcDistTable, calculatedMeasure)
}
#### Standardise the magnitude distance so that the measure ranges from 0 to 1
minMaxDist <- calcDistTable[, .(minDist = min(measure), maxDist = max(measure)),
by = c("Store1", "YEARMONTH")]
distTable <- merge(calcDistTable, minMaxDist, by = c("Store1", "YEARMONTH"))
distTable[, magnitudeMeasure := 1 - (measure - minDist)/(maxDist - minDist)]
finalDistTable <- distTable[, .(mag_measure = mean(magnitudeMeasure)), by =
.(Store1, Store2)]
return(finalDistTable)
}
#### Over to you! Use the function you created to calculate correlations against store 77 using total sales and number of customers.
#### Hint: Refer back to the input names of the functions we created.
trial_store <- 77
corr_nCustomers <- calculateCorrelation(preTrialMeasures, quote(nCustomers), trial_store)
#### Then, use the functions for calculating magnitude.
magnitude_nSales <- calculateMagnitudeDistance(preTrialMeasures, quote(totSales), trial_store)
magnitude_nCustomers <- calculateMagnitudeDistance(preTrialMeasures, quote(nCustomers), trial_store)
#### Create a combined score composed of correlation and magnitude, by first merging the correlations table with the magnitude table.
#### Hint: A simple average on the scores would be 0.5 * corr_measure + 0.5 * mag_measure
corr_weight <- 0.5
score_nSales <- merge(corr_weight, corr_nSales, by = c("Store1", "Store2"))[, scoreNSales := corr_measure * corr_weight + mag_measure * (1- corr_weight)]
score_nSales <- merge(corr_nSales, magnitude_nSales by = c("Store1", "Store2"))[, scoreNSales := corr_measure * corr_weight + mag_measure * (1- corr_weight)]
score_nSales <- merge(corr_nSales, magnitude_nSales, by = c("Store1", "Store2"))[, scoreNSales := corr_measure * corr_weight + mag_measure * (1- corr_weight)]
score_nCustomers <- merge(corr_nCustomers, magnitude_nCustomers, by = c("Store1", "Store2"))[, scoreNSales := corr_measure * corr_weight + mag_measure * (1- corr_weight)]
#### Combine scores across the drivers by first merging our sales scores and customer scores into a single table
score_Control <- merge(score_nSales, score_nCustomers, by = c("Store1", "Store2"))
score_Control[, finalControlScore := scoreNSales * 0.5 + scoreNCust * 0.5]
score_nSales <‐ merge(corr_nSales, magnitude_nSales, by = c("Store1",
score_nSales <- merge(corr_nSales, magnitude_nSales, by = c("Store1",
"Store2"))[, scoreNSales := corr_measure * corr_weight + mag_measure * (1‐
#### Create a combined score composed of correlation and magnitude, by first merging the correlations table with the magnitude table.
#### Hint: A simple average on the scores would be 0.5 * corr_measure + 0.5 * mag_measure
corr_weight <‐ 0.5
#### Create a combined score composed of correlation and magnitude, by first merging the correlations table with the magnitude table.
#### Hint: A simple average on the scores would be 0.5 * corr_measure + 0.5 * mag_measure
corr_weight <- 0.5
score_nSales <- merge(corr_nSales, magnitude_nSales, by = c("Store1",
"Store2"))[, scoreNSales := corr_measure * corr_weight + mag_measure * (1-
corr_weight)]
score_nCustomers <- merge(corr_nCustomers, magnitude_nCustomers, by =
c("Store1", "Store2"))[, scoreNCust := corr_measure * corr_weight +
mag_measure * (1- corr_weight)]
Now we have a score for each of total number of sales and number of customers.
Let's combine the two via a simple average.
```{r}
#### Combine scores across the drivers by first merging our sales scores and customer scores into a single table
score_Control <- merge(score_nSales, score_nCustomers, by = c("Store1", "Store2"))
score_Control[, finalControlScore := scoreNSales * 0.5 + scoreNCust * 0.5]
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <‐ score_Control[Store1 == trial_store,][order(‐finalControlScore)][2, Store2]
control_store
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <- score_Control[Store1 == trial_store,][order(‐finalControlScore)][2, Store2]
control_store
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <- score_Control[Store1 == trial_store,][order(-finalControlScore)][2, Store2]
control_store
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <‐ score_Control[Store1 == trial_store, ][order(-finalControlScore)][2, Store2]
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <- score_Control[Store1 == trial_store, ][order(-finalControlScore)][2, Store2]
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <- score_Control[Store1 == trial_store, ][order(-finalControlScore)][2, Store2]
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <- score_Control[Store1 == trial_store, ][order(-finalControlScore)][2, Store2]
#### Combine scores across the drivers by first merging our sales scores and customer scores into a single table
score_Control <- merge(score_nSales, score_nCustomers, by = c("Store1", "Store2"))
score_Control[, finalControlScore := scoreNSales * 0.5 + scoreNCust * 0.5]
#### Combine scores across the drivers by first merging our sales scores and customer scores into a single table
score_Control <- merge(score_nSales, score_nCustomers, by = c("Store1", "Store2"))
score_Control[, finalControlScore := scoreNSales * 0.5 + scoreNCust * 0.5]
The store with the highest score is then selected as the control store since it is
most similar to the trial store.
```{r}
#### Select control stores based on the highest matching store (closest to 1 but
#### not the store itself, i.e. the second ranked highest store)
#### Over to you! Select the most appropriate control store for trial store 77 by finding the store with the highest final score.
control_store <- score_Control[Store1 == trial_store, ][order(-finalControlScore)][2, Store2]
control_store
#### Visual checks on trends based on the drivers
measureOverTimeSales <- measureOverTime
pastSales <- measureOverTimeSales[, Store_type := ifelse(STORE_NBR == trial_store, "Trial",
ifelse(STORE_NBR == control_store, "Control", "Other stores"))
][, totSales := mean(totSales), by = c("YEARMONTH",
"Store_type")
][, TransactionMonth := as.Date(paste(YEARMONTH %/%
100, YEARMONTH %% 100, 1, sep = "-"), "%Y-%m-%d")
][YEARMONTH < 201903 , ]
ggplot(pastSales, aes(TransactionMonth, totSales, color = Store_type)) +
geom_line() +
labs(x = "Month of operation", y = "Total sales", title = "Total sales by month")
ggplot(pastCustomers, aes(TransactionMonth, numberCustomers, color =
Store_type)) +
geom_line() +
labs(x = "Month of operation", y = "Total number of customers", title = "Total
↪ number of customers by month")
#### Visual checks on customer count trends by comparing the trial store to the control store and other stores.
#### Hint: Look at the previous plot.
measureOverTimeCusts <- measureOverTime
pastCustomers <- measureOverTimeCusts[, Store_type := ifelse(STORE_NBR ==
trial_store, "Trial",
ifelse(STORE_NBR == control_store,  "Control", "Other stores"))][, numberCustomers := mean(nCustomers), by = c("YEARMONTH", "Store_type")][, TransactionMonth := as.Date(paste(YEARMONTH %/%
100, YEARMONTH %% 100, 1, sep = "‐"), "%Y‐%m‐%d")][YEARMONTH < 201903 , ]
ggplot(pastCustomers, aes(TransactionMonth, numberCustomers, color =
Store_type)) +
geom_line() +
labs(x = "Month of operation", y = "Total number of customers", title = "Total
↪ number of customers by month")
#### Scale pre-trial control sales to match pre-trial trial store sales
scalingFactorForControlSales <- preTrialMeasures[STORE_NBR == trial_store &
YEARMONTH < 201902, sum(totSales)]/preTrialMeasures[STORE_NBR == control_store &
YEARMONTH < 201902, sum(totSales)]
#### Apply the scaling factor
measureOverTimeSales <- measureOverTime
scaledControlSales <- measureOverTimeSales[STORE_NBR == control_store, ][ ,
controlSales := totSales * scalingFactorForControlSales]
#### Calculate the percentage difference between scaled control sales and trial sales
percentageDiff <- merge(scaledControlSales[, c("YEARMONTH", "controlSales")],
measureOverTime[STORE_NBR == trial_store, c("totSales",
"YEARMONTH")],
by = "YEARMONTH")[, percentageDiff :=
abs(controlSales - totSales)/controlSales]
#### As our null hypothesis is that the trial period is the same as the pre-trial period, let's take the standard deviation based on the scaled percentage difference in the pre-trial period
stdDev <- sd(percentageDiff[YEARMONTH < 201902 , percentageDiff])
#### Note that there are 8 months in the pre-trial period
#### hence 8 - 1 = 7 degrees of freedom
degreesOfFreedom <- 7
#### to check whether the hypothesis is statistically significant.
#### Hint: The test statistic here is (x - u)/standard deviation
percentageDiff[, tValue := (percentageDiff - 0)/stdDev][, TransactionMonth := as.Date(paste(YEARMONTH %/% 100, YEARMONTH %% 100, 1, sep = "‐"), "%Y‐%m‐%d")
][YEARMONTH < 201905 & YEARMONTH > 201901, .(TransactionMonth, tValue)]
#### Find the 95th percentile of the t distribution with the appropriate
#### degrees of freedom to compare against
qt(0.95, df = degreesOfFreedom)
